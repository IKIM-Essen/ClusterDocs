{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"IKIM Scientific Computing","text":"<p>This website contains documentation for the scientific computing infrastructure at the Institute for AI in Medicine (IKIM) in Essen. The documentation is geared towards researchers and students that aim to run scientific experiments on the cluster. See Getting Started for general instructions.</p> <p>The image below shows members of IKIM assembling the cluster </p>"},{"location":"#about","title":"About","text":"<p>The sources of this documentation can be found on GitHub and we encourage contribution.</p>"},{"location":"#intro","title":"Intro","text":"<p>We believe it is important to know a thing or two about the underlying computer and network infrastructure to be effective. We note this will also limit your frustration level. We do not provide extensive documentation, but rather jumping off points and short best practice info on your setup and your procedures.</p> <p>By configuring your environment correctly you can make your job easier. Start off by learning the basics of the Slurm cluster. For info on getting the right execution environment set up for your code, check out Mamba/Conda or install your software in a container as you would with Docker. Some users will benefit from using interactive Jupyter Notebooks.</p> <p>Most computes involve storing, accessing and moving data, as well as transferring data into the cluster.</p> <p>If you pay attention to a few details in organizing your compute things will go a lot smoother. We recommend using reproducible approaches with SnakeMake to structure your compute.</p> <p>We note that typically compute resources are available but the lack of good computing practices leads to contention for IO resources, which in turn slow everyone down.</p> <p>We are adding things to the documentation to aid our users, please familiarize yourself with it. Also check out the lessons learned.</p>"},{"location":"access/","title":"Requesting access","text":""},{"location":"access/#generating-the-ssh-key","title":"Generating the SSH key","text":"<p>To get access to the IKIM computing infrastructure you need an SSH key.</p>"},{"location":"access/#linux-and-macos","title":"Linux and MacOS","text":"<p>Use the command below to create your SSH key. When prompted, make sure to choose a strong passphrase and save the passphrase in your password manager.</p> <pre><code>ssh-keygen -t ed25519 -f ~/.ssh/id_ikim\n</code></pre> Example: output of SSH-keypair generation.  When executing the command above, you should should see output similar to this:  <pre><code>Generating public/private ed25519 key pair.\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /Users/&lt;user&gt;/.ssh/id_ikim\nYour public key has been saved in /Users/&lt;user&gt;/.ssh/id_ikim.pub\nThe key fingerprint is:\nSHA256:PQyNrogYs001Y0IlsG75teDBFVlDmd7xSJPNI1lrQr4 user@&lt;host&gt;\nThe key's randomart image is:\n+--[ED25519 256]--+\n|..o...++o.*.     |\n| o . ..o+X +.    |\n|. . =.. =oBo.    |\n|. o+.o o *+.     |\n|o+.+ .  SE+      |\n|.Bo.+...   .     |\n|o oo...          |\n|                 |\n|                 |\n+----[SHA256]-----+\n</code></pre>  Note that two files were created in your home directory in the `.ssh` subdirectory:  <pre><code>$ ls ~/.ssh\nconfig  id_ikim  id_ikim.pub  known_hosts\n</code></pre>  - `~/.ssh/id_ikim` - This is your private SSH key. Treat this file like a password. Do not share it with anyone. - `~/.ssh/id_ikim.pub` - This is your public SSH key. This should be shared with your project coordinator. You can open it with any text editor.  The contents of `~/.ssh/id_ikim.pub` look similar to this:  <pre><code>$ cat ~/.ssh/id_ikim.pub\nssh-ed25519 [long random string] &lt;user&gt;@&lt;host&gt;\n</code></pre>"},{"location":"access/#windows","title":"Windows","text":"<p>Windows requires the OpenSSH client, which has been permanently integrated into the system since the October 2018 Windows 10 update. If this is not available (so you can't use the command <code>ssh-keygen</code>), simply follow the instructions at the following link: OpenSSH</p> <p>Open Powershell and Use the command below to create your SSH key. When prompted, make sure to choose a strong passphrase and save the passphrase in your password manager.</p> <pre><code>ssh-keygen -t ed25519\n</code></pre> <p>You can accept the default settings for the path, but name the key <code>id_ikim</code> when prompted. The private and public key will appear by default at <code>C:\\Users\\&lt;username&gt;\\.ssh\\</code> (note that the name of this directory begins with a dot and is therefore invisible by default) as <code>id_ikim</code> and <code>id_ikim.pub</code> respectively.</p>"},{"location":"access/#sending-the-request","title":"Sending the request","text":"<p>Please send the public key along with following contact details to your project coordinator:</p> <ul> <li>First name</li> <li>Last name</li> <li>Email address (domain uk-essen.de or uni-due.de if available)</li> <li>Public SSH key (<code>~/.ssh/id_ikim.pub</code>)</li> </ul> <p>Afterwards, an account will be created for you in the central user management. When this is done, you should be able to SSH into the cluster.</p>"},{"location":"accessing-storage/","title":"Accessing cluster storage on Windows and Mac clients","text":"<p>You can access storage on the cluster directly from your laptop (or if need be from a desktop computer). This is not intended for transferring large amounts of data, but rather for editing a spreadsheet needed for the next experiment that is stored on a server in the IKIM network and is mounted to the lab instrument.</p> <p>Secure access to file server storage in the cluster is available via sshFS. It is not intended for large file transfer. See transfer for options for large file transfer.</p> <p>We will first ensure all the bits are in place, test the setup and finally set your system up to automatically connect to directories on the file servers.</p> <p>While you can mosly cut-copy-paste, you still need to replace user, group and project names with your own.</p> <p>Below is a resonable configuration for a home directory, a group directory and one project directory.</p> name remote directory local directory Parameters Home directory /homes/juser/ /Users/joe/remote/ <code>-odefer_permissions,volname=HOMES-DIR</code> Group directory /groups/experts/ /Users/joe/group/ <code>-defer_permissions,volname=GROUPDIR</code> Project directory /project/workwithme/ /Users/joe/project/ <code>-odefer_permissions,volname=PROJECTDIR</code> <p>Please insert your username, groupname and project name respectively. You can add more directories if you so desire.</p>"},{"location":"accessing-storage/#howto-for-windows-clients","title":"HowTo for Windows Clients","text":"<p>These instructions assume you are the administrator of your Windows computer and that it is running the latest fully patched version of Windows.</p> <ol> <li> <p>Download and Install \u2013 WinFsp</p> <p>Only install the core elements as indicated here</p> <p></p> </li> <li> <p>From the same page, download SSHFS-win(x64) and install</p> </li> <li> <p>Download and install the latest version of</p> <p>SSHFS-win manager</p> </li> <li> <p>Open a txt file in the .ssh folder (located in the user folder), enter the following, and save it as a .ps1 file</p> <pre><code>Start-Process ssh -WindowStyle Hidden -ArgumentList \"-fN\", \"-L\", \"6666:shellhost.ikim.uk-essen.de:22\", \"shellhost\"\n</code></pre> </li> <li> <p>Open SSHFS-win Manager \u2013 click on the Add connection button, enter the following, and save</p> <p></p> </li> <li> <p>Open Task Scheduler</p> </li> <li> <p>Select Action &gt; Create Task</p> </li> </ol> <p>7.1 In the General Tab \u2013 give a name to the action and select Run with highest privileges</p> <p></p> <p>7.2 Under the Trigger tab \u2013 click new at the bottom left and select the option in the figure</p> <p></p> <p>7.3 Under the Action tab, click new at the bottom left</p> <ul> <li>Action - Start a Program</li> <li>In Program/script: option enter powershell.exe</li> <li> <p>In Add Arguments: option enter the following</p> <pre><code>-ExecutionPolicy Bypass -File \"indicate the location of the ps1 file here\" \n</code></pre> </li> </ul> <p>7.4 Under Conditions tab</p> <p></p> <p>7.5 Under Settings Tab</p> <p></p> <p>7.6 Click OK</p> <p>Your task should be available in the task scheduler library \u2013 right click and select run.</p> <p>Then go to SSHFS-win manager and click the connect button (the round button with the plug and socket). You should now have the IKIM drive mapped in your file explorer. Now, the connection should be automatically triggered every time you connect to a network, for example, after restarting.</p>"},{"location":"accessing-storage/#howto-for-mac-clients","title":"HowTo for Mac Clients","text":"<p>We provide a step-by-step guide for setting up secure remote storage access.</p> <ol> <li> <p>Install MacFuse and SSHFS:  Follow the instructions at MacFuse. Please note that this will require at least one reboot and some level of  attention to details. It should take about 3-5 minutes to complete.</p> </li> <li> <p>Ensure sure that your <code>~/.ssh/config</code> file is set up correctly</p> </li> </ol> <p>Check our guide on setting up the ssh config.</p> <ol> <li>Verify your <code>~/.ssh/config</code> file  Note: You will not need to do this to use the storage, but if this does not work, there is something wrong with your ssh setup.</li> </ol> <p><code>ssh shellhost</code></p> <p>If this does not work, please check the guide above.</p> <ol> <li>Create an empty directory Mac for temporary purposes</li> </ol> <p>Open a  Terminal window and type</p> <p><code>mkdir ~/remote</code></p> <p>Or use the Finder to create this directories. (Please note this guide assumes there is not already a file or directory by this name.)</p> <ol> <li>Mount your homes directory into your local environment</li> </ol> <p>In your Terminal execute the command below to mount, after replacing \"juser\" with your own username for the cluster.</p> <p><code>sshfs juser@shellhost:/homes/juser $HOME/remote/     -odefer_permissions,volname=HOMES-DIR</code></p> <p>We supply a volume name (<code>volname</code>) to describe the directory, you can modify the name to your liking.</p> <ol> <li> <p>check if things work Type <code>mount | fgrep ikim</code> and you should see something link</p> <p><code>juser@shellhost.ikim.uk-essen.de:/homes/juser on /Users/joe/remote (macfuse, nodev, nosuid,  synchronous, mounted by juser)</code></p> </li> </ol> <p>You are now connected to your home directory on the file server.</p> <p>You can now either use command line tools like <code>ls</code> or use the Mac Finder to browse the directory. But we are not done, if you close your laptop or log out, the network drive will not re-appear. So we need to ensure its available permanently.</p> <ol> <li>Remove the temporary bits</li> </ol> <p>Before we automate the setup, we need to clean up. You can use the Finder to unmount the directory (right-click) and remove the now empty directory that we created earlier.</p> <p>Or you can type:</p> <pre><code>```\ndiskutil unmount force ~/remote\nrmdir ~/remote\n```\n</code></pre> <ol> <li> <p>Now set up permanent access to the server</p> <ol> <li>Download ConnectMeNow Depending on your computer use Apple Silicon or Intel to download the software.</li> </ol> </li> <li> <p>Follow this guide on how to use the software to automate mounting.</p> <ol> <li> <p>Ensure you set the following parameters are set for share</p> <p> * select SSHFS as ShareType * set Server Address: <code>shellhost</code> * set the Path to your homes-directory e.g. <code>/homes/juser</code></p> </li> <li> <p>Ensure you set the following parameters are set for Advanced Options</p> <p> * ensure that the menu title makes sense, we suggest using <code>homes-dir</code>, etc. * ensure that under Advanced settings <code>Ping before mount</code> is disabled * ensure under MountStyle you use <code>-o follow_symlinks,defer_permissions,volname=homes-dir</code></p> </li> </ol> </li> <li> <p>Success.</p> </li> </ol> <p>If everything worked well so far, you should now have set up one or more Share definition in ConnectMeNow. The software will appear as an entry in the menu bar of your Mac like this .</p> <p>You have now successfully enabled file secure server access to the server using your existing SSH configuration.</p> <p>Please note that this is not the correct option to transfer very large files.</p> <ol> <li> <p>FAQs</p> <ol> <li> <p>If you experience a permisssion denied response, you forgot the <code>-odefer_permissions</code> parameter</p> </li> <li> <p>ConnectMeNow disappeared, check the menu bar.</p> </li> </ol> </li> </ol>"},{"location":"apptainer/","title":"Containers","text":"<p>Linux containers (LXC) provide a means to create a binary file that contains an execution environment (all software required to execute some code). Rather than installing those so called dependencies and then installing the software itself, the entire software system is contained in a single file. A container can be generated from a single file. Find a discussion of container flavors here.</p>"},{"location":"apptainer/#working-with-containers-with-apptainer","title":"Working with containers (with Apptainer)","text":"<p>Apptainer is the container system installed on the Slurm cluster. In contrast to Docker, it does not grant the user elevated privileges and is well-suited to multi-user environments. Nonetheless, it can run containers from Docker images as long as they do not depend on intrinsic root privileges such as binding to host ports below 1024.</p> <p>This document is not a comprehensive guide on Apptainer. To learn more, see the official manual.</p>"},{"location":"apptainer/#basic-commands","title":"Basic commands","text":"<p>Apptainer provides the following commands for launching containers:</p> <ul> <li><code>apptainer run</code>: executes the default startup command.</li> <li><code>apptainer exec</code>: executes a custom command.</li> <li><code>apptainer shell</code>: executes an interactive shell.</li> <li><code>apptainer instance start</code>: executes a background service.</li> </ul> <p>The above commands support mostly the same set of options.</p>"},{"location":"apptainer/#executing-a-docker-image","title":"Executing a Docker image","text":"<p>When using the prefix <code>docker://</code>, Apptainer pulls the image from Docker Hub.</p> <pre><code>apptainer run docker://alpine\n</code></pre> <p>The default execution model of Apptainer is different from Docker's. The container filesystem in Apptainer is read-only, although a number of paths such as <code>/tmp</code>, the user's home and the current directory are mounted read-write from the host into the container. Additionally, the default container user is the host user rather than root. The following examples demonstrate the effects of this behavior.</p> <pre><code># When creating a file in the container, it is owned by the current user both in the container and on the host.\nalice@c1:~$ apptainer exec \\\n    docker://alpine \\\n    sh -c 'touch hello &amp;&amp; ls -l hello'\n-rw-rw-r--    1 alice   alice           0 Apr 13 10:07 hello\nalice@c1:~$ ls -l hello\n-rw-rw-r-- 1 alice alice 0 Apr 13 10:07 hello\n</code></pre> <pre><code># Non-mounted paths are read-only.\nalice@c1:~$ apptainer exec \\\n    docker://alpine \\\n    apk add busybox-extras\nERROR: Unable to lock database: Read-only file system\nERROR: Failed to open apk database: Read-only file system\n</code></pre> <p>The immutability of non-mounted paths, in combination with executing as the host user, makes it convenient to run multiple containers in parallel on the cluster with easy access to NFS storage. If this is not desired, other modes of operation are listed below.</p>"},{"location":"apptainer/#bind-mounting","title":"Bind-mounting","text":"<p>If a Docker image expects to be able to write in specific locations, they can simply be mounted in the container while preserving the read-only mode for the rest of the filesystem. For example:</p> <pre><code>mkdir ~/pgrun ~/pgdata\napptainer run \\\n    --bind ~/pgdata:/var/lib/postgresql/data \\\n    --bind ~/pgrun:/var/run/postgresql \\\n    --env POSTGRES_PASSWORD=secret \\\n    docker://postgres\n</code></pre>"},{"location":"apptainer/#writable-tmpfs","title":"Writable tmpfs","text":"<p>The option <code>--writable-tmpfs</code> creates a writable area that allows making changes to the container filesystem. The default size is 16 MiB, therefore it is only suitable for small writes such as PID or state-tracking files. Any changes are lost when the container exits.</p> <pre><code>mkdir ~/pgdata\napptainer run \\\n    --bind ~/pgdata:/var/lib/postgresql/data \\\n    --writable-tmpfs \\\n    --env POSTGRES_PASSWORD=secret \\\n    docker://postgres\n</code></pre>"},{"location":"apptainer/#sandbox","title":"Sandbox","text":"<p>Apptainer can switch to a full read-write model by combining sandbox directories with fakeroot mode. A sandbox is a filesystem tree in a directory on the host. When executing a container from a sandbox, the filesystem can be made writable. Fakeroot mode makes the user appear as root in the container, thereby allowing complete access to the container filesystem.</p> <pre><code># Create a sandbox on local storage.\nalice@c1:~$ apptainer build --sandbox /local/work/mysandbox docker://alpine\nalice@c1:~$ ls /local/work/mysandbox\nbin  dev  environment  etc  home  lib  media  mnt  opt  proc  root  run  sbin  singularity  srv  sys  tmp  usr  var\n\n# Install a package in the sandbox.\nalice@c1:~$ apptainer exec \\\n    --writable \\\n    --fakeroot \\\n    /local/work/mysandbox \\\n    apk add busybox-extras\n</code></pre> <p>Fakeroot mode cannot work properly if the sandbox directory is on NFS. For best results, sandboxes should be placed on local storage as shown in the example above.</p> <p>Depending on the changes made to a sandbox, it might not be possible to delete it from the host as a regular user, although it can always be deleted from a fakeroot container:</p> <pre><code># Remove the contents of the sandbox.\napptainer exec --fakeroot --bind /local/work/mysandbox docker://alpine rm -R /local/work/mysandbox\n\n# Remove the leftover empty directory.\nrmdir /local/work/mysandbox\n</code></pre>"},{"location":"apptainer/#using-gpus","title":"Using GPUs","text":"<p>The <code>--nv</code> option instructs Apptainer to add GPU support to the container.</p> <pre><code>apptainer exec --nv docker://nvcr.io/nvidia/cuda:12.1.0-base-ubuntu22.04 /usr/bin/nvidia-smi\n</code></pre> <p>Apptainer doesn't have an equivalent of the <code>--gpus</code> option from the NVIDIA Docker runtime. The environment variable <code>CUDA_VISIBLE_DEVICES</code> should be used to control GPU visibility inside the container. See Targeting GPU nodes with Slurm.</p>"},{"location":"computing/","title":"Efficient cluster Computing patterns","text":"<p><code>Nota bene</code>: The cluster can accomodate a finite amount of IO at any one point. IO is where computing breaks. At the node level and certainly at the cluster level.</p> <p>The cluster provices a number of storage facilities described here. Make sure you are familiar with them and you understand the nature of local vs. remote file systems.</p>"},{"location":"computing/#local-file-systems","title":"Local file systems","text":"<p>Every Linux machine has at least one local filesystem. Ours typically have a minimum of 2 hard drives. One is dedicated to system things (and some scratch space for the system) and is thus quite full and quite limited. The other is a data drive mounted at /local/work.</p> <p>The local file systems are limited in size, but provide more IO bandwidth than network or remote drives.</p> <p>Note: Anything that is <code>/homes</code>, <code>/groups</code> or <code>projects</code> is a remote file system. See here for some details.</p>"},{"location":"computing/#remote-file-systems-and-what-not-to-do","title":"Remote file systems and what not to do","text":"<p>Remote file systems are a great idea, for some applications and some use cases. Computing on remote data is typically a really bad idea. You will waste a lot of cycles on your computing device waiting for data to be moved onto your computing devices RAM and subsequently any CPU or GPU caches. While the file servers are equipped to handle a lot of IO, dozens or hundreds of clients will easily overwhelm them.</p> <p>A good pattern is to stream your data to local disk (maybe from an object store) compute on it and later move or copy your result files onto a network share (never write to a network directly). This way you can avoid being slowed down by other users. We recommend an object store as this can handle more users in parallel without slowing down.</p>"},{"location":"conda/","title":"Mamba/Conda","text":"<p>Mambaforge provides <code>mamba</code> and <code>conda</code> on the cluster. Users who prefer to manage their own installation can install a Conda distribution in their home directory.</p> <p>This document is not a comprehensive guide on Conda. To learn more, see the official manual. The conda intro provides a good starting point.</p>"},{"location":"conda/#using-conda-with-slurm","title":"Using Conda with Slurm","text":"<p>Submitting a slurm job which includes <code>conda activate</code> will result in the following error:</p> <pre><code>$ srun -N1 conda activate myenv\n\nCommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\nTo initialize your shell, run\n\n    $ conda init &lt;SHELL_NAME&gt;\n</code></pre> <p>This happens because <code>conda activate</code> is made available as a shell function which is not preserved when slurm executes a job.</p> <p>The suggestion of executing <code>conda init</code> does not apply to Slurm. Instead, an environment can be activated with one of the following methods:</p> <ul> <li>Execute <code>conda activate</code> while logged into a submission node, then submit the job:</li> </ul> <pre><code>$ conda activate myenv\n(myenv) $ srun -N1 env | grep CONDA_PREFIX\nCONDA_PREFIX=/homes/user/.conda/envs/myenv\n</code></pre> <ul> <li>Execute <code>eval \"$(conda shell.bash hook)\"</code> as part of the slurm job in order to make <code>conda activate</code> available:</li> </ul> <pre><code>$ cat run.sh\n#!/usr/bin/env bash\n#SBATCH -N1\neval \"$(conda shell.bash hook)\"\nconda activate myenv\nsrun -N1 env | grep CONDA_PREFIX\n\n$ sbatch run.sh\n\n$ cat slurm-120651.out\nCONDA_PREFIX=/homes/user/.conda/envs/myenv\n</code></pre>"},{"location":"first-steps/","title":"Welcome to the cluster","text":"<p>You now have connected successfully the cluster.  You can now use Mamba to configure your environment or Apptainer to execute a binary object.</p> <p>You should be connected to one of the computers we have designated</p>"},{"location":"first-steps/#how-to-find-a-computing-device","title":"How to find a computing device","text":"<p>You will be connected to one of a number of interactive servers (SHELLHOST). Please do not perform extensive computing on these nodes. Use our resource manager to obtain access to the resources you need.</p>"},{"location":"first-steps/#where-to-store-your-data","title":"Where to store your data?","text":"<p>There are several locations where you can store data on the cluster:</p> <ul> <li>Your home directory (<code>/homes/&lt;username&gt;/</code>): This directory is only for personal data such as configuration files. Anything related to work or that should be visible to other people should not reside here.</li> <li>Project directory (<code>/projects/&lt;project_name&gt;/</code>): This location should be used for data related to your project. If you are starting a project, ask your project coordinator to create a directory and provide a list of participating users. Note that you cannot simply list all project directories via <code>ls /projects</code>; instead, you need to specify the full path, such as: <code>ls /projects/dso_mp_ws2021/</code></li> <li>Public dataset directory (<code>/projects/datashare</code>): A world-readable location for datasets for which no special access rights are required. To lower the risk of data loss, each user can write only in a subdirectory corresponding to their research group. For example, a user which belongs to group <code>tio</code> should add new datasets in <code>/projects/datashare/tio</code> but can browse and read throughout <code>/projects/datashare</code>.</li> <li>Group directory (<code>/groups/&lt;group_name&gt;</code>): This is the appropriate place for any data that should be shared within an IKIM research group. In student projects you will most likely not need group directories.</li> </ul> <p>All of the above directories (homes, projects, groups) are shared with other hosts on the cluster through the network file system (NFS). This is convenient: sharing data between hosts becomes effortless and your data is stored redundantly on the file server.</p> <p>Also see the storage for details and also info on performance. If you need to transfer data, reading transfer</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Welcome the IKIM cluster documentation. The goal of this document is to give you enough background to work on the IKIM cluster. It is not meant as a general introduction to remote computing services. We will refer to external sources where necessary.</p> <p>If you have any questions, please reach out to your project coordinator for help.</p>"},{"location":"getting-started/#getting-cluster-access","title":"Getting cluster access","text":""},{"location":"getting-started/#generating-the-ssh-key-on-linux-and-macos","title":"Generating the SSH key on Linux and MacOS","text":"<p>To get access to the IKIM computing infrastructure you need an SSH key. Use the command below to create your SSH key. When prompted, make sure to choose a strong passphrase and save the passphrase in your password manager.</p> <pre><code>ssh-keygen -t ed25519 -f ~/.ssh/id_ikim\n</code></pre> <p>Please send the public key along with following contact details to your project coordinator:</p> <ul> <li>First name</li> <li>Last name</li> <li>Email address (domain uk-essen.de or uni-due.de if available)</li> <li>Public SSH key (<code>~/.ssh/id_ikim.pub</code>)</li> </ul> <p>Afterwards, an account will be created for you in the central user management. When this is done, you should be able to SSH into the cluster.</p> Example: output of SSH-keypair generation.  When executing the command above, you should should see output similar to this:  <pre><code>Generating public/private ed25519 key pair.\nEnter passphrase (empty for no passphrase):\nEnter same passphrase again:\nYour identification has been saved in /Users/&lt;user&gt;/.ssh/id_ikim\nYour public key has been saved in /Users/&lt;user&gt;/.ssh/id_ikim.pub\nThe key fingerprint is:\nSHA256:PQyNrogYs001Y0IlsG75teDBFVlDmd7xSJPNI1lrQr4 user@&lt;host&gt;\nThe key's randomart image is:\n+--[ED25519 256]--+\n|..o...++o.*.     |\n| o . ..o+X +.    |\n|. . =.. =oBo.    |\n|. o+.o o *+.     |\n|o+.+ .  SE+      |\n|.Bo.+...   .     |\n|o oo...          |\n|                 |\n|                 |\n+----[SHA256]-----+\n</code></pre>  Note that two files were created in your home directory in the `.ssh` subdirectory:  <pre><code>$ ls ~/.ssh\nconfig  id_ikim  id_ikim.pub  known_hosts\n</code></pre>  - `~/.ssh/id_ikim` - This is your private SSH key. Treat this file like a password. Do not share it with anyone. - `~/.ssh/id_ikim.pub` - This is your public SSH key. This should be shared with your project coordinator. You can open it with any text editor.  The contents of `~/.ssh/id_ikim.pub` look similar to this:  <pre><code>$ cat ~/.ssh/id_ikim.pub\nssh-ed25519 [long random string] &lt;user&gt;@&lt;host&gt;\n</code></pre>"},{"location":"getting-started/#generating-the-ssh-key-on-windows","title":"Generating the SSH key on Windows","text":"<p>Windows requires the OpenSSH client, which has been permanently integrated into the system since the October 2018 Windows 10 update. If this is not available (so you can't use the command <code>ssh-keygen</code>), simply follow the instructions at the following link: OpenSSH</p> <p>Open Powershell and Use the command below to create your SSH key. When prompted, make sure to choose a strong passphrase and save the passphrase in your password manager.</p> <pre><code>ssh-keygen -t ed25519\n</code></pre> <p>You can accept the default settings for the path, but name the key <code>id_ikim</code> when prompted. The key pair (public and private) will appear by default at <code>C:\\Users\\&lt;username&gt;\\.ssh\\</code> (note that the name of this directory begins with a dot and is therefore invisible by default) inside your user directory. When done, send the contents of the public key with the file name id_ikim.pub with contact details (name and email adress) to your project coordinator.</p>"},{"location":"getting-started/#setting-up-your-ssh-configuration","title":"Setting up your SSH configuration","text":"<p>To provide the appropriate parameters for the connection, create a file at <code>~/.ssh/config</code> (on Windows: <code>C:\\Users\\&lt;username&gt;\\.ssh\\config</code>) and copy the snippet below, replacing <code>$USERNAME</code> appropriately.</p> <pre><code>Host *\n  AddKeysToAgent yes\n  CanonicalizeHostname yes\n\nHost ikim\n  HostName login.ikim.uk-essen.de\n  User $USERNAME\n  IdentityFile ~/.ssh/id_ikim\n  ForwardAgent yes\n\nHost g?-? c? c?? c??? shellhost\n  Hostname %h.ikim.uk-essen.de\n  User $USERNAME\n  IdentityFile ~/.ssh/id_ikim\n  ProxyJump ikim\n  ForwardAgent yes\n</code></pre>"},{"location":"getting-started/#test-your-ssh-login","title":"Test your SSH login","text":"<p>Try the example below to test that your SSH client is properly configured:</p> <pre><code>ssh ikim\n</code></pre> <p>If it succeeds, type <code>exit</code> to log out. The <code>ikim</code> host must be used only for ssh authentication and not for computational work; in fact, users should not log into it directly. Using the provided configuration file, ssh will automatically \"jump through\" the <code>ikim</code> host to reach the compute nodes.</p> <p>For instructions on using the compute nodes, see the section What software is available on the IKIM cluster?</p> <p>If the login test fails, please run the command below and send the output to your project coordinator for help.</p> <pre><code>ssh -v ikim\n</code></pre>"},{"location":"getting-started/#what-hardware-is-available-on-the-ikim-cluster","title":"What hardware is available on the IKIM cluster?","text":"<p>The cluster has two sets of servers: 120 nodes for CPU-bound tasks and 10+ nodes for GPU-bound tasks. At this moment, not all of these nodes are available for general computation tasks. However, more will be added in future. The following hardware is installed in the servers:</p> <ul> <li>CPU nodes (<code>c1</code> - <code>c120</code>): Each with 192GB RAM, 2 CPU Intel, 1 SSD for system and 1 SSD for data (2TB).</li> <li>GPU nodes (<code>g1-1</code> - <code>g1-10</code>): Each with 6 NVIDIA RTX 6000 GPUs, 1024GB RAM, 2 CPU AMD, 1 SSD for system (1TB) and 2 NVMe for data (12TB configured as RAID-0).</li> <li>GPU node (<code>g2-1</code>): One node with 8 NVIDIA A100 80G GPUs, 2TB RAM, 2 AMD EPYC CPUs (256 logical processors), 1 SSD for system (1TB) and 2 NVMe for data (30TB configured as RAID-0).</li> </ul> <p>A subset of these nodes are deployed as a Slurm cluster. Unless instructed otherwise, you should interact with worker nodes using Slurm.</p>"},{"location":"getting-started/#what-software-is-available-on-the-ikim-cluster","title":"What software is available on the IKIM cluster?","text":"<p>Short answer: Everything under the sun. Familiarize yourself with our job scheduler, then install software using either a package manager or build a container. Compared to package managers like Conda, containers provide greater isolation from the host operating system and are recommended for producing repeatable workflows.</p> <p>Example: To install scikit-learn all you need to do is</p> <pre><code>conda create -n sklearn-env -c conda-forge scikit-learn\nconda activate sklearn-env\n</code></pre> <p>Conda and its siblings (anaconda and mamba) provide access to thousands of software packages, you can set up your required software by yourself and even have multiple environments. The conda intro provides a good starting point.</p>"},{"location":"getting-started/#where-to-store-your-data","title":"Where to store your data?","text":"<p>There are several locations where you can store data on the cluster:</p> <ul> <li>Your home directory (<code>/homes/&lt;username&gt;/</code>): This directory is only for personal data such as configuration files. Anything related to work or that should be visible to other people should not reside here.</li> <li>Project directory (<code>/projects/&lt;project_name&gt;/</code>): This location should be used for data related to your project. If you are starting a project, ask your project coordinator to create a directory and provide a list of participating users. Note that you cannot simply list all project directories via <code>ls /projects</code>; instead, you need to specify the full path, such as: <code>ls /projects/dso_mp_ws2021/</code></li> <li>Public dataset directory (<code>/projects/datashare</code>): A world-readable location for datasets for which no special access rights are required. To lower the risk of data loss, each user can write only in a subdirectory corresponding to their research group. For example, a user which belongs to group <code>tio</code> should add new datasets in <code>/projects/datashare/tio</code> but can browse and read throughout <code>/projects/datashare</code>.</li> <li>Group directory (<code>/groups/&lt;group_name&gt;</code>): This is the appropriate place for any data that should be shared within an IKIM research group. In student projects you will most likely not need group directories.</li> </ul> <p>All of the above directories (homes, projects, groups) are shared with other hosts on the cluster through the network file system (NFS). This is convenient: sharing data between hosts becomes effortless and your data is stored redundantly on the file server.</p> <p>Also see the storage for details and also info on performance. If you need to transfer data, reading transfer</p>"},{"location":"getting-started/#tips-on-working-with-remote-computing-services","title":"Tips on Working with remote computing services","text":"<ul> <li>Unix Crash Course</li> <li>Another Unix Course</li> <li>Tactical tmux: The 10 Most Important Commands</li> <li>How To Use Linux Screen</li> <li>Git Book</li> <li>Conda</li> </ul>"},{"location":"jupyter/","title":"Jupyter Notebook Workflow","text":"<p>The Jupyter Notebook is an open-source web application that allows creating and sharing documents that contain live code, equations, visualizations and narrative text. Uses include: data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. Visit Jupyter.org for more information.</p>"},{"location":"jupyter/#starting-a-jupyter-server-instance","title":"Starting a Jupyter server instance","text":"<p>To launch a Jupyter instance on the Slurm cluster, first log into a Slurm submission node:</p> <pre><code>ssh shellhost\n</code></pre> <p>Next, either create a conda environment with the package <code>notebook</code> from the channel <code>conda-forge</code> or install the package in an existing environment.</p> <pre><code>conda create -n myproject -c conda-forge notebook\nconda activate myproject\n</code></pre> <p>The Jupyter startup command can now be submitted to Slurm. Here is a minimal example of an sbatch script called <code>jupyter.sh</code> which launches Jupyter on a worker node with 8 allocated CPU cores and an 8-hour deadline:</p> <pre><code>$ cat jupyter.sh\n#!/usr/bin/env bash\n\n#SBATCH --time 08:00:00\n#SBATCH --cpus-per-task=8\n\njupyter notebook --ip 0.0.0.0 --no-browser\n</code></pre> <p>Submit the job and follow the output:</p> <pre><code>$ sbatch jupyter.sh\nSubmitted batch job 1234\n$ tail -F slurm-1234.out\n</code></pre> <p>The hostname, port number and authorization token are displayed upon startup. For example:</p> <pre><code>[I 06:30:19.290 NotebookApp] Serving notebooks from local directory: /homes/jan\n[I 06:30:19.290 NotebookApp] Jupyter Notebook 6.4.5 is running at:\n[I 06:30:19.290 NotebookApp] http://g1-7.ikim.uk-essen.de:8888/       ?token=d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9\n[I 06:30:19.290 NotebookApp]  or http://127.0.0.1:8888/       ?token=d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9\n[I 06:30:19.290 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice    to     skip confirmation).\n[C 06:30:19.296 NotebookApp]\n\n   To access the notebook, open this file in a browser:\n       file:///homes/jan/.local/share/jupyter/runtime/nbserver-426528-open.html\n   Or copy and paste one of these URLs:\n       http://g1-7.ikim.uk-essen.de:8888/?token=d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9\n    or http://127.0.0.1:8888/?token=d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9\n</code></pre> <p>The output above contains the following information:</p> <ul> <li>The hostname of the worker node: <code>g1-7</code>.</li> <li>The port number: <code>8888</code>.</li> <li>The authorization token: <code>d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9</code>.</li> </ul>"},{"location":"jupyter/#connecting-to-the-remote-jupyter-server","title":"Connecting to the remote Jupyter server","text":""},{"location":"jupyter/#from-a-browser-on-the-local-machine","title":"From a browser on the local machine","text":"<p>Worker nodes in the cluster are not exposed to the internet. In order to reach a cluster node from a local browser, an SSH tunnel through the login node must be launched. Open a terminal and execute the following command, replacing the hostname and port number from the example with the actual values:</p> <pre><code>ssh -N -L 8888:g1-7:8888 ikim\n</code></pre> <p>This command creates an SSH tunnel such that all traffic to <code>localhost:8888</code> passes through the host <code>ikim</code> and reaches the destination <code>g1-7:8888</code>. The terminal will simply hang, indicating that the tunnel has been created.</p> <p>Open a browser and paste the URL provided by Jupyter. Since the traffic needs to flow through the tunnel, replace the target host with <code>localhost</code>:</p> <pre><code>http://localhost:8888/?token=d6e1289f41b1433b557c06dd78c9d716180dc2a8ea61e8a9\n</code></pre> <p>The Jupyter landing page should appear:</p> <p></p> <p>To verify that the notebook is running on the remote host and within the conda environment:</p> <p></p>"},{"location":"jupyter/#from-visual-studio-code","title":"From Visual Studio Code","text":"<p>Connect to a Slurm submission node as described in Connect to a remote host. When prompted for the target host, type <code>shellhost</code>. From there, connect to the Jupyter server as described in Connect to a remote Jupyter server. When prompted for the URL, simply copy and paste the URL displayed in the output from <code>jupyter notebook</code>. As opposed to the browser method, in this case the connection originates from inside the cluster and there's no need to set up port forwarding.</p>"},{"location":"patterns/","title":"Good patterns","text":"<p>This document contains a list of dos and don'ts. The intent is to emphasize lessons learned.</p>"},{"location":"patterns/#using-local-and-network-storage-appropriately","title":"Using local and network storage appropriately","text":"<p>The place where your data is stored during computation is a major factor in performance. Make sure that any and all data you use for computing resides on local storage (<code>/local/work</code>). Failure to do so will slow your compute down (often by a factor of 1000) and impact all other users.</p> <p>Also make sure that results are written to local storage and later moved or copied to a shared filesystem.</p>"},{"location":"patterns/#vscode-prevent-expensive-full-text-searches","title":"VSCode: Prevent Expensive Full Text Searches","text":"<p>We notice some users running processes taking a lot of CPU and network IO with <code>rg</code> (short for ripgrep). This code is executed by Visual Studio Code when running full text searches (e.g., cmd + shift + f). Be aware that such searches may be very expensive when you have many files in your project, especially when your code is on an NFS location (e.g., <code>/homes</code> or <code>/groups</code>)</p> <p>Therefore, you should limit the search space. Two options:</p> <ol> <li>(recommended) VSCode by default excludes files from search that match patterns in <code>.gitignore</code> and <code>.ignore</code>.  So best practice is to have a language-specific gitignore in the project to avoid searches over directories which have tens of thousands of small files (like venv, node_modules, dist, etc.). Most likely you also want to exclude <code>data/</code>.</li> <li>Disable search. Open vscode settings, search for <code>search.exclude</code>, add pattern (<code>**</code> and <code>*</code>). This will exclude all files from search.</li> </ol>"},{"location":"performance/","title":"Resources and Performance -- The status, the DOs and DON'Ts","text":"<p>The performance of your computation depends primarily on:</p> <ol> <li>I/O (Input/Output)</li> <li>RAM</li> <li>Choice of technology stack</li> <li>Hardware</li> </ol> <p>\u2014 in that order of importance.</p>"},{"location":"performance/#cluster-overview","title":"Cluster Overview","text":"<p>As of late 2024, the cluster offers:</p> <ul> <li>~10,000 CPU cores</li> <li>~100,000 GPU cores</li> <li>35+ Terabytes of RAM</li> <li>3+ Petabytes of storage</li> </ul> <p>However, these impressive numbers are meaningless unless you optimize your computation with the appropriate tools and strategies. The machines are interconnected via 10 Gbit/sec Ethernet, while dedicated file servers utilize multiple 100 Gbit/sec connections. Data can move from file servers at ~1 GB/sec, making data transfer speed and local RAM availability critical.</p>"},{"location":"performance/#key-factors-in-optimizing-performance","title":"Key Factors in Optimizing Performance","text":""},{"location":"performance/#1-inputoutput-io","title":"1. Input/Output (I/O)","text":"<p>Your computation\u2019s performance is far more dependent on I/O patterns than on raw hardware capabilities. Here are critical considerations:</p> <ul> <li>Local storage over remote access: Always move your data to fast, local storage before starting computation. Running computations on remotely mounted data (e.g., directly from a file server) can be up to 100\u00d7 slower than using local storage.</li> <li>Data transfer rates: While nodes have 10 Gbit/sec Ethernet connections (~1 GB/sec), file servers must service hundreds of nodes. As such, they cannot guarantee high-performance, storage-agnostic I/O.</li> <li>Streaming computation: Streaming data directly to a node and computing on-the-fly is an advanced approach that requires detailed knowledge of I/O bandwidth and the computational throughput of your device. Use this method only when confident in its feasibility.</li> </ul>"},{"location":"performance/#2-ram-memory","title":"2. RAM (Memory)","text":"<p>Efficient use of RAM is vital for performance. Key considerations include:</p> <ul> <li>Avoid paging and swapping: When your computation exceeds available RAM, the system will \u201cpage\u201d data to local disk storage, which can slow performance by up to 1,000\u00d7. Estimating your RAM requirements is critical.</li> <li>Divide data into subsets: If your computation requires more RAM than available, consider splitting your dataset into manageable subsets to avoid paging.</li> <li>Node RAM capacities: Nodes vary in RAM availability, with many offering 200 GB and fewer providing 1 TB. Overestimating your requirements and queuing for high-RAM nodes may increase wait times without guaranteeing success.</li> </ul>"},{"location":"performance/#3-technology-stack","title":"3. Technology Stack","text":"<p>When computations exceed a single CPU core\u2019s capacity, ensure your software stack supports parallel processing technologies like OpenMP or MPI. Here\u2019s how:</p> <ul> <li>Conda-installed software: Most software distributed via Conda is pre-configured to leverage multi-core processing through OpenMP.</li> <li>Manually installed software: If installing software manually, verify that multi-core or parallel processing support is enabled during installation.</li> </ul>"},{"location":"performance/#4-hardware-selection","title":"4. Hardware Selection","text":"<p>Your choice of hardware should align with your software\u2019s capabilities and your computational needs:</p> <ul> <li>CPU vs. GPU: While GPUs can be significantly faster, their effectiveness depends on your software stack\u2019s compatibility. Not all code is GPU-optimized.</li> <li>RAM requirements: Select nodes based on your RAM needs:</li> <li>Overuse of high-RAM nodes (e.g., 1 TB) can lead to longer wait times.</li> <li>Understanding and optimizing your RAM usage often reduces bottlenecks.</li> <li>Divide datasets where possible to mitigate excessive RAM demands.</li> </ul>"},{"location":"performance/#summary","title":"Summary","text":"<p>To achieve optimal performance:</p> <ul> <li>Prioritize local storage for I/O-intensive tasks.</li> <li>Plan RAM usage to avoid paging.</li> <li>Ensure your software stack supports multi-core processing or GPU acceleration.</li> <li>Choose hardware configurations tailored to your workload\u2019s requirements.</li> </ul> <p>Understanding and managing these factors will help you maximize the efficiency of your computations on the cluster.</p>"},{"location":"performance/#dont-do-this","title":"Don't do this","text":"<ul> <li> <p>Don't run computations that will access a lot of files in /projects or /groups (and of course /homes), at least don't act surprised if they are really slow and other users complain about the systems IO performance going down dramatically.</p> </li> <li> <p>Don't run computations that require 10 MB (10 megabytes) of RAM blocking a 1 Terabyte (1000000 Megabytes) node</p> </li> </ul>"},{"location":"resources/","title":"Resources","text":""},{"location":"resources/#available-hardware","title":"Available hardware","text":"<p>The cluster has two sets of servers: 120 nodes for CPU-bound tasks and 10+ nodes for GPU-bound tasks. At this moment, not all of these nodes are available for general computation tasks. However, more will be added in future. The following hardware is installed in the servers:</p> <ul> <li>CPU nodes (<code>c1</code> - <code>c120</code>): Each with 192GB RAM, 2 CPU Intel, 1 SSD for system and 1 SSD for data (2TB).</li> <li>GPU nodes (<code>g1-1</code> - <code>g1-10</code>): Each with 6 NVIDIA RTX 6000 GPUs, 1024GB RAM, 2 CPU AMD, 1 SSD for system (1TB) and 2 NVMe for data (12TB configured as RAID-0).</li> <li>GPU node (<code>g2-1</code>): One node with 8 NVIDIA A100 80G GPUs, 2TB RAM, 2 AMD EPYC CPUs (256 logical processors), 1 SSD for system (1TB) and 2 NVMe for data (30TB configured as RAID-0).</li> </ul> <p>A subset of these nodes are deployed as a Slurm cluster. Unless instructed otherwise, you should interact with worker nodes using Slurm.</p>"},{"location":"resources/#available-software","title":"Available software","text":"<p>Short answer: Everything under the sun. You can install software yourself using either a package manager or build / run a container. Containers can even be used e.g. to run a different operating system should you need to. To avoid resource contention we recommend using our resource manager.</p> <p>Example: To install scikit-learn all you need to do is</p> <pre><code>conda create -n sklearn-env -c conda-forge scikit-learn\nconda activate sklearn-env\n</code></pre> <p>Conda and its siblings (anaconda and mamba) provide access to thousands of software packages, you can set up your required software by yourself and even have multiple environments. The conda intro provides a good starting point.</p>"},{"location":"s3/","title":"Storage on the IKIM cluster","text":""},{"location":"s3/#classes-of-storage","title":"Classes of storage","text":""},{"location":"s3/#s3-storage","title":"S3 storage","text":"<p>Unlike local and NFS storage, S3 storage has a advantages and some downsides. We provide in house S3 storage.</p> <p>The main advantage of so called object store is speed, S3 storage is very fast but does not provide the same storage abstractions as local or NFS. By not providing a POSIX style view of the data (Meaning that instead of typing ls, you have to use a special client to browse the data) and not providing any locking and/or coordination semantics, S3 servers can be fast and reliable.</p> <p>Data will have to be copied from S3 storage to a local disk for use, however copying is usually done a line speed allowing for 1Gigabyte per second transfers. Importantly results should be copied back from local storage (leaving out temporary data). Frequently data is packed into zip or tar.gz files before uploading.</p>"},{"location":"s3/#getting-started-with-s3","title":"Getting started with S3","text":""},{"location":"s3/#sample-use-cases","title":"Sample use cases","text":""},{"location":"s3/#using-s3-for-data-transfer","title":"Using S3 for data transfer","text":""},{"location":"slurm/","title":"Slurm","text":"<p>This document gives an overview of Slurm to get you started. To learn more, also see the official manual.</p>"},{"location":"slurm/#entrypoint","title":"Entrypoint","text":"<p>The entry point to the Slurm cluster is a set of submission nodes under the name <code>shellhost</code>. Submission nodes must be used exclusively to log in and submit jobs (inline commands or scripts) to worker nodes.</p> <p>To connect to a submission node:</p> <pre><code>ssh shellhost\n</code></pre> <p>Worker nodes are divided in groups called partitions in Slurm terminology. The default partition is made up of general-purpose CPU nodes.</p>"},{"location":"slurm/#client-tools","title":"Client tools","text":"<p>This is a brief overview of the main commands available on the submission nodes. They can also be invoked from worker nodes as part of a submitted job.</p>"},{"location":"slurm/#sinfo","title":"sinfo","text":"<p>Use <code>sinfo</code> to display a summary of the worker nodes.</p>"},{"location":"slurm/#squeue","title":"squeue","text":"<p>Use <code>squeue</code> to display a list of scheduled jobs. It's generally useful to execute</p> <pre><code>squeue -l\n</code></pre> <p>to display additional information such as the time limit.</p>"},{"location":"slurm/#srun","title":"srun","text":"<p>Use <code>srun</code> to submit a job interactively. The output is displayed on the terminal.</p>"},{"location":"slurm/#example-obtaining-a-shell-on-a-node","title":"Example: obtaining a shell on a node","text":"<p>The following example opens a shell on a worker node. The node is selected automatically by Slurm.</p> <pre><code># A deadline is specified to avoid leaving a hanging session if the user doesn't terminate it.\nsrun --time=01:00:00 --pty bash -i\n</code></pre> <p>It's also possible to connect to worker nodes directly via ssh. See Example: connect to a worker node.</p>"},{"location":"slurm/#example-requesting-resources","title":"Example: requesting resources","text":"<p>A simple <code>srun</code> command is executed by default on a single worker node, using a single CPU core. Several options are available for configuring the allocated resources. In the following example, Slurm allocates 32 CPU cores by picking a worker node with enough free cores.</p> <pre><code>srun --cpus-per-task=32 python3 script.py\n</code></pre>"},{"location":"slurm/#example-environment-inheritance","title":"Example: environment inheritance","text":"<p>When Slurm executes a job, it inherits the caller's environment and current directory. Slurm also sets several environment variables such as <code>SLURM_JOB_ID</code> and <code>SLURM_JOB_NAME</code>. The same applies to the other submission commands <code>sbatch</code> and <code>salloc</code>.</p> <p>To illustrate, let's create a script called <code>job.sh</code> and execute it via <code>srun</code>.</p> <pre><code>$ pwd\n/homes/alice/workdir\n\n$ cat job.sh\n#!/usr/bin/env sh\necho \"my job ran on $(date)\"\necho \"working directory: $(pwd)\"\necho \"FOO=$FOO\"  # this variable is not defined in the script\necho \"job id: $SLURM_JOB_ID\"  # this variable is defined by Slurm\necho $(hostname)\n\n# Define an environment variable and execute the job.\n$ FOO=bar srun sh job.sh\nmy job ran on Thu May  4 15:25:10 UTC 2023\nworking directory: /homes/alice/workdir\nFOO=bar\njob id: 340273\nc5.ikim.uk-essen.de\n</code></pre>"},{"location":"slurm/#sbatch","title":"sbatch","text":"<p>Use <code>sbatch</code> to submit a script. The output is written to a file in the current directory. All <code>srun</code> options apply to <code>sbatch</code> as well and can be included in the script itself with the special comment syntax <code>#SBATCH</code>.</p> <p><code>sbatch</code> allocates the requested resources, then executes the script on the first of the allocated nodes. To use the allocated resources effectively, <code>srun</code> must be invoked within the script, otherwise only one node is used.</p>"},{"location":"slurm/#example-submitting-a-job-with-sbatch","title":"Example: submitting a job with sbatch","text":"<p>Let's create a script called <code>job.sh</code> with a couple of <code>srun</code> calls and execute it via <code>sbatch</code>. Slurm redirects standard output to the text file <code>slurm-[job_id].out</code>.</p> <pre><code># Create a script which allocates 3 nodes and submits two job steps using srun.\n$ cat job.sh\n#!/usr/bin/env sh\n#SBATCH --nodes 3\n#SBATCH --time 01:00:00\nsrun hostname\nsrun echo hello\n\n# Execute the script.\n$ sbatch ./job.sh\nSubmitted batch job 340264\n\n# Display the output. The jobs steps were executed simultaneously on all 3 nodes.\n$ cat slurm-340264.out\nc7.ikim.uk-essen.de\nc23.ikim.uk-essen.de\nc20.ikim.uk-essen.de\nhello\nhello\nhello\n</code></pre> <p>The following example shows the effect of an equivalent script without <code>srun</code>.</p> <pre><code># Create a script which allocates 3 nodes, then executes two commands without srun.\n$ cat job.sh\n#!/usr/bin/env sh\n#SBATCH --nodes 3\n#SBATCH --time 01:00:00\nhostname\necho hello\n\n# Execute the script.\n$ sbatch ./job.sh\nSubmitted batch job 340266\n\n# Display the output. The commands were simply executed on the first assigned node.\n$ cat slurm-340266.out\nc7.ikim.uk-essen.de\nhello\n</code></pre>"},{"location":"slurm/#example-connect-to-a-worker-node","title":"Example: connect to a worker node","text":"<p>Users can connect via ssh to worker nodes on which they have at least one running job. This is typically useful for launching monitoring (<code>htop</code>, <code>nvidia-smi</code>, etc.) or debugging tools.</p> <p>After launching a job, the allocated node can be quickly discovered by passing <code>-u username</code> to <code>squeue</code>. For example:</p> <pre><code>$ squeue -u alice\nJOBID PARTITION   NAME    USER ST   TIME  NODES NODELIST(REASON)\n 1234      IKIM   job1   alice  R   10:30      1 c11\n 1245      IKIM   job2   alice  R    3:05      1 c23\n</code></pre> <p>To connect to one of the listed nodes, open a new shell on the local workstation (not on the submission node) and execute:</p> <pre><code>ssh c11\n</code></pre> <p>The ssh session is incorporated into one of the running jobs on the target node: when the job terminates, the ssh session terminates as well.</p> <p>This feature is meant for monitoring and debugging. Do not use it for work that could be submitted via slurm instead.</p> <p>Interactive shells can also be launched on any worker node using <code>srun</code>, regardless of running jobs. See Example: obtaining a shell on a node. The <code>srun</code> method allocates one CPU core on the target node, therefore it doesn't work if the node is already fully allocated.</p>"},{"location":"slurm/#example-create-a-pipeline-of-jobs","title":"Example: create a pipeline of jobs","text":"<p>The <code>--dependency</code> option allows creating a tree of interdependent jobs such that downstream jobs start only after upstream jobs terminate with a certain outcome.</p> <pre><code># This example submits a job, then two parallel downstream jobs which depend on\n# on successful completion of the upstream job:\n#\n#      Job 10\n#     /      \\\n# Job 11    Job 12\n#\n# Submit the upstream job.\n$ sbatch preprocess.sh\nSubmitted batch job 10\n\n# Submit a downstream job configured to wait in the queue until job 10\n# terminates successfully.\n$ sbatch --dependency afterok:10 train1.sh\nSubmitted batch job 11\n\n# Submit another downstream job.\n$ sbatch --dependency afterok:10 train2.sh\nSubmitted batch job 12\n\n# Check the queue.\n$ squeue\n    JOBID PARTITION           NAME    USER ST    TIME  NODES NODELIST(REASON)\n       12      IKIM      train2.sh   alice PD    0:00      1 (Dependency)\n       11      IKIM      train1.sh   alice PD    0:00      1 (Dependency)\n       10      IKIM  preprocess.sh   alice  R    0:15      1 c20\n</code></pre> <p>From a resource allocation standpoint, the individual jobs are separate: they can request different resources and might be scheduled on different nodes unless specified otherwise.</p> <p>For the full dependency syntax, see the --dependency section in the official manual.</p>"},{"location":"slurm/#salloc","title":"salloc","text":"<p><code>salloc</code> allocates resources that can be used by subsequent invocations of <code>srun</code> or <code>sbatch</code>. By default, after allocating the resources <code>salloc</code> opens a shell on the current node: any <code>srun</code> or <code>sbatch</code> commands issued in this shell will use the allocated resources. When the user terminates the shell, the allocation is relinquished.</p> <p>The following example requests allocates a certain number of nodes for a limited amount of time. Note that the assigned nodes are not allocated exclusively unless specified, for example by requesting all available CPU cores.</p> <pre><code>$ salloc --nodes=3 --time=01:00:00\nsalloc: Granted job allocation 340227\n\n$ srun hostname\nc5.ikim.uk-essen.de\nc6.ikim.uk-essen.de\nc7.ikim.uk-essen.de\n\n$ exit\nsalloc: Relinquishing job allocation 340227\nsalloc: Job allocation 340227 has been revoked.\n</code></pre> <p>If a command is supplied to <code>salloc</code>, it is executed instead of opening a shell. The command runs on the current node, therefore it should include <code>srun</code> or <code>sbatch</code>.</p> <p>To request only an allocation without executing a specific command right away, execute <code>salloc</code> with <code>--no-shell</code>. As soon as the allocation is granted, a job ID is displayed in the output. The allocation can then be used by invoking <code>srun</code> or <code>sbatch</code> and passing the job ID to the <code>--jobid</code> option. The allocation stays active until either the <code>--time</code> runs out or it gets terminated via <code>scancel</code>.</p>"},{"location":"slurm/#example-allocate-resources-for-future-use","title":"Example: allocate resources for future use","text":"<p>The following example allocates 3 GPUs on the same node for 8 hours. At any point within this time frame, jobs can be executed on the allocation.</p> <pre><code>$ salloc --no-shell --time=08:00:00 --partition GPUampere --nodes=1 --gpus 3\nsalloc: Granted job allocation 486835\n\n# A shell or any other job can now be submitted on allocation 486835.\n# This can be done as many times as desired until the session is canceled or times out.\n$ srun --jobid=486835 --pty bash -i\n</code></pre> <p>This approach is helpful for executing jobs in bursts over a period of time without losing the claim on certain resources. For instance, a user planning a bug-fixing session (short runs interspersed with code changes) on a GPU-equipped node can allocate a GPU for a few hours instead of submitting each job to the queue individually.</p> <p>See Targeting GPU nodes to learn more about requesting GPUs.</p>"},{"location":"slurm/#scontrol","title":"scontrol","text":"<p>Use <code>scontrol</code> to display detailed information about a job such as the allocated resources and the times of submission/start.</p> <pre><code>scontrol show jobid -dd [job_id]\n</code></pre>"},{"location":"slurm/#scancel","title":"scancel","text":"<p>Use <code>scancel [job_id]</code> to cancel or terminate a job. Use <code>squeue</code> to display job IDs.</p>"},{"location":"slurm/#targeting-gpu-nodes","title":"Targeting GPU nodes","text":"<p>GPU nodes are available in a non-default partition named after the corresponding NVIDIA GPU architecture. The <code>--partition</code> option must be specified to target them.</p> <p>When executing GPU workloads, the <code>--gpus N</code> option should always be specified to let slurm assign <code>N</code> GPUS automatically. More specifically, Slurm automatically selects a suitable worker node, picks the specified number of GPUs randomly among the unassigned ones and sets the environment variable <code>CUDA_VISIBLE_DEVICES</code> accordingly.</p> <pre><code># In this example, Slurm exposes 2 GPUs from a node in the GPUampere partition via CUDA_VISIBLE_DEVICES.\n# The script must not override CUDA_VISIBLE_DEVICES!\n# The number of requested CPU cores should be proportional to the number of GPUs.\n# A deadline of 1 day and 12 hours is specified to let other users know when the GPUs will be available again.\nsrun --partition GPUampere --gpus 2 --cpus-per-gpu=4 --time=1-12 train.py\n</code></pre> <p>The end-user script can then refer to specific devices by using library-specific mechanisms. For example, after passing <code>--gpus N</code> to Slurm, Pytorch maps the assigned CUDA devices to indices <code>0</code> through <code>N-1</code>: <code>cuda:0</code>, <code>cuda:1</code>, etc. Since CUDA_VISIBLE_DEVICES is managed by Slurm, end-user scripts must not modify it.</p> <p>If the option <code>--gpus</code> is omitted, slurm does not set the <code>CUDA_VISIBLE_DEVICES</code> variable. This does not mean that GPUs are hidden: the job will have access to all GPUs on the node, even the ones assigned to slurm jobs from other users.</p>"},{"location":"slurm/#job-submission-etiquette","title":"Job submission etiquette","text":""},{"location":"slurm/#setting-a-deadline","title":"Setting a deadline","text":"<p>If a job is expected to run continuously for many hours, a deadline should be specified with the option <code>--time</code>, even if just an overestimation. This information is especially valuable when all worker nodes are occupied as it allows other users to predict when their job will be scheduled. Accepted time formats include <code>minutes</code>, <code>minutes:seconds</code>, <code>hours:minutes:seconds</code>, <code>days-hours</code>, <code>days-hours:minutes</code> and <code>days-hours:minutes:seconds</code>.</p> <p>It's good practice to always specify a deadline when opening a shell (<code>srun --pty bash</code>). This avoids the \"hanging session\" issue that occurs if the user forgets to log out or loses the connection abruptly.</p>"},{"location":"slurm/#balance-between-cpu-cores-and-gpus","title":"Balance between CPU cores and GPUs","text":"<p>When requesting GPUs with <code>--gpus</code>, attention should be paid to the ratio of CPU cores to GPUs by passing an appropriate value to <code>--cpus-per-gpu</code>. For example, if a node has 6 GPUs and 48 CPU cores in total, it's advisable to request no more than <code>--cpus-per-gpu=8</code>.</p> <p>The amount of CPU cores and GPUs on a node can be determined with <code>scontrol show node</code>.</p>"},{"location":"slurm/#breaking-up-jobs-at-checkpoints","title":"Breaking up jobs at checkpoints","text":"<p>A large workflow that runs for multiple days occupying several CPU cores or GPUs typically has a checkpoint system that saves its state at regular intervals. Instead of implementing such a workflow as a single job, it can be broken up into a sequence of jobs that end by saving a checkpoint and start by loading the previous checkpoint. This increases the fairness of the queue by allowing other jobs to be scheduled in between. See the pipeline example for details.</p>"},{"location":"slurm/#cheat-sheet","title":"Cheat Sheet","text":"What Command Connect to submission nodes. Only use for submission. <code>ssh shellhost</code> Show workers <code>sinfo</code> Show schedule <code>squeue -l</code> My jobs <code>squeue -u $(whoami)</code> Submit interactive job <code>srun --time=01:00:00 --cpus-per-task=1 --pty bash -i</code> Submit batch job <code>sbatch job.sh</code> Submit batch job w/o shell script <code>sbatch --wrap=\"python -m ...\"</code> Target GPU nodes <code>sbatch --partition=GPUampere,GPUhopper --gpus=1 --cpus-per-gpu=4 --time=01:00:00 job.sh</code> Allocate resources for later <code>salloc [ARGS] --time=01:00:00</code> Show job info <code>scontrol show jobid -dd [JOB_ID]</code> Show assigned GPUs <code>scontrol show jobid -dd [JOB_ID] \\| grep IDX</code> Cancel job <code>scancel [JOB_ID]</code>"},{"location":"snakemake/","title":"Snakemake","text":"<p>The Snakemake workflow management system is a tool to create transparent reproducible and scalable data analyses.</p> <ul> <li>Workflows are described via a human readable, Python based language.</li> <li>They can be seamlessly scaled to server, cluster, grid and cloud environments, without the need to modify the workflow definition.</li> <li>Snakemake workflows can entail a description of required software (via Mamba/Conda or container images), which will be automatically deployed to any execution environment.</li> <li>Snakemake can automatically create portable, server-less interactive HTML reports that contain all requested results and connect them to data provenance information like code and parameters.</li> <li>On our cluster, Snakemake is configured to automatically avoid malicious IO patterns. No need to manually copy to the local workdir for avoiding NFS stress, Snakemake takes care of these things automatically.</li> <li>The Snakemake homepage gives a high-level overview on the most important features: https://snakemake.github.io</li> <li>In case of any issues or questions, reach out for Prof. Johannes K\u00f6ster (IKIM 4th floor).</li> </ul>"},{"location":"snakemake/#news","title":"News","text":"<ul> <li>2025/09/01: Snakemake 9.10.1 includes an important performance improvement for gluster (our storage). This should lead to much less small file operations (in particular no mv-after-create, which is an issue on gluster). It should dramatically reduce I/O errors when using Snakemake.</li> <li>2024/03/12: We upgraded the cluster-wide Snakemake profile <code>ikim</code> to requre Snakemake 8.6.0. Please update your Snakemake installation as shown below.</li> </ul>"},{"location":"snakemake/#installation","title":"Installation","text":"<p>Currently, our cluster setup requires Snakemake &gt;= 8.6.0. Snakemake can be easily installed using the mamba package manager that is preinstalled on the cluster. First, ssh into a shellhost machine</p> <pre><code>ssh shellhost\n</code></pre> <p>Then create a snakemake environment via the preinstalled mamba package manager:</p> <pre><code>mamba create -c conda-forge -c bioconda --name snakemake snakemake snakemake-storage-plugin-fs snakemake-executor-plugin-slurm\n</code></pre> <p>It is recommended to execute your analyses via Slurm, for maintenance and performance reasons (while Snakemake would also work without it). Via a dedicated Snakekemake profile we have ensured that Snakemake transparently uses Slurm when you execute it from a <code>shellhost</code> node, and runs locally if on a non-shellhost node.</p>"},{"location":"snakemake/#update","title":"Update","text":"<p>It is a good idea to keep the <code>snakemake</code> environment up to date in order to receive bug fixes and feature updates. For updating to the latest versions, run</p> <pre><code>mamba update --name snakemake snakemake snakemake-storage-plugin-fs snakemake-executor-plugin-slurm\n</code></pre>"},{"location":"snakemake/#usage","title":"Usage","text":"<p>Assumptions:</p> <ul> <li>You are on one of the <code>shellhost</code> machines (<code>ssh shellhost</code>).</li> <li>You are inside of a working directory that contains a Snakemake workflow (either a file <code>Snakefile</code> or <code>workflow/Snakefile</code> in the same dir). It is very important that this directory is an NFS directory that is shared between all cluster nodes (it may thus not start with <code>/local</code>, but instead with e.g. <code>/projects</code>).</li> <li>You have completed above installation steps.</li> </ul> <p>Since your Snakemake workflow might run for a longer time, you usually want it to be independent of the current ssh session (otherwise, the Snakemake process would be killed when the session is closed or disconnected). Therefore, we recommend to first generate a so-called tmux session via</p> <pre><code>tmux new -s SESSIONNAME\n</code></pre> <p>with <code>SESSIONNAME</code> being a reasonable name under which you can remember your intended Snakemake run. For a full summary of all tmux functionality, see here. In addition, note for youself the name of the host (<code>hostname -a</code>), so that you can later come back to the same in case you close the ssh session youself out or are disconnected. This is necessary because <code>shellhost</code> is a DNS name that is associated with multiple physical machines and you can never know to which machine you care connected when doing <code>ssh shellhost</code> (this happens because we want to distribute the load of many active users across several machines). In such a case, you can come back to the same host (say, <code>HOSTNAME</code>) and open the tmux session via</p> <pre><code>ssh HOSTNAME\ntmux attach -t SESSIONNAME\n</code></pre> <p>Inside of the tmux session, you can use Snakemake on the slurm cluster by running</p> <pre><code>nice snakemake --jobs N\n</code></pre> <p>with <code>N</code> being the number of jobs you want to run in parallel at most, and it will automatically submit the jobs to the slurm cluster. The <code>nice</code> command before the <code>snakemake</code> invocation ensures that your Snakemake process on the <code>shellhost</code> has less priority than any user interaction. This is important to ensure that the <code>shellhost</code> remains responsive. In order to perform a dry-run (i.e. just see the plan), which is highly recommended before actually executing, run</p> <pre><code>snakemake -n\n</code></pre> <p>For a full tutorial on Snakemake, please check out the official Snakemake tutorial.</p>"},{"location":"snakemake/#further-hints","title":"Further hints","text":"<ul> <li>Make sure to follow the best-practices.</li> <li>If you and your colleagues have common tasks that often reoccur in workflows, you can save a lot of time by activating Snakemake's between workflow caching.</li> </ul>"},{"location":"ssh-setup/","title":"Setting up your ssh client","text":""},{"location":"ssh-setup/#configuring-the-ssh-client-on-your-system","title":"Configuring the ssh client on your system","text":"<p>On your laptop do these steps.</p> <p>To provide the appropriate parameters for the connection, create a file at <code>~/.ssh/config</code> (on Windows: <code>C:\\Users\\&lt;username&gt;\\.ssh\\config</code>) and copy the snippet below, replacing <code>$USERNAME</code> appropriately.</p> <pre><code>Host *\n  AddKeysToAgent yes\n  CanonicalizeHostname yes\n\nHost ikim\n  HostName login.ikim.uk-essen.de\n  User $USERNAME\n  IdentityFile ~/.ssh/id_ikim\n  ForwardAgent yes\n\nHost g?-? c? c?? c??? shellhost\n  Hostname %h.ikim.uk-essen.de\n  User $USERNAME\n  IdentityFile ~/.ssh/id_ikim\n  ProxyJump ikim\n  ForwardAgent yes\n</code></pre>"},{"location":"ssh-setup/#test-your-ssh-login","title":"Test your SSH login","text":"<p>Try the example below to test that your SSH client is properly configured:</p> <pre><code>ssh ikim\n</code></pre> <p>If it succeeds, type <code>exit</code> to log out. The <code>ikim</code> host must be used only for ssh authentication and not for computational work; in fact, users should not log into it directly. Using the provided configuration file, ssh will automatically \"jump through\" the <code>ikim</code> host to reach the compute nodes.</p> <p>If the login test fails, please run the command below and send the output to your project coordinator for help.</p> <pre><code>ssh -v ikim\n</code></pre>"},{"location":"storage/","title":"Storage on the IKIM cluster","text":"<p>The cluster has a number of options for retrieving and storing data. They have vastly different performance characteristics and greatly influence the time required to complete your computational analyses.</p>"},{"location":"storage/#classes-of-storage","title":"Classes of storage","text":"<p>Not all storage locations are alike and it is worth your while to understand their specific properties.</p>"},{"location":"storage/#local-storage-on-the-system-partition","title":"Local storage on the system partition","text":"<p>The local storage on each node typically consists of a system partition and a data partition. The system partition is used for the operating system, the configuration, swap files, pre-installed software. Most directories on the node are read-only to users.</p> location purpose user read-write status comment /etc/ configuration read-only /var/ temporary files read-only /var/tmp user-generated temporary files read-write local disk /tmp/ user-generated temporary files, deleted on reboot read-write local disk"},{"location":"storage/#local-storage-on-the-data-partition","title":"Local storage on the data partition","text":"<p>For some operations the NFS comes with unnecessary overhead. Therefore, the path <code>/local/work</code> is available for creating files and directories that reside on the data partition of the current host. This location should only be used for quick testing, preliminary experimentation and intermediate output. As soon as you need your files saved, move them to <code>/projects</code> or <code>/groups</code>. Local-only files are not backed up and can be deleted without notice.</p> <p>Here are tips on writing programs, scripts, containers, etc. that make good use of network resources:</p> <ul> <li>Read inputs from and write the final results to <code>/projects</code> or <code>/groups</code>.</li> <li>Write intermediate output to <code>/local/work</code>.</li> </ul>"},{"location":"storage/#nfs-storage","title":"NFS storage","text":"<p>Read operations on network storage (<code>/projects</code>, <code>/groups</code>) are cached transparently on local storage in the data partition. Generally speaking, your first access to a dataset will be slightly slower than usual due, but any subsequent access will be made from local storage.</p> <p>The file server has a 10Gib (10Gbs, 10 gigabit per second connection to the entire cluster. As a consequence each node can access a fraction of 10Gib, in the worst case a tiny fraction. However we note that a 250MB (megabyte) file will need a fraction of a second to transfer from the server to the client. This rather brilliant performance stats drastically change if and when random IO (as in not streaming large files, write-locking files, etc.) enter the equation. Those complex operations are best left to local disk.</p> <p>As a consequence, using local files or cached files is a good idea to ensure good runtime performance.</p> <p>Three different storage locations exist on the file server:</p> location purpose user read-write status comment /projects/ project data read-write not listable /groups/ group files read-write not listable /homes user home directory read-write not cached <p>Each user has a private home-directory. The contents of which are private to the userm typically no data relevant to any other user, project or your PI should be stored here.</p> <p>The projects directory provides a means to generate project specific storage, typically associated with a linux group shared by all members of the project. Thus <code>/projects/abc</code> is shared only by members of the project <code>abc</code>. We note that by using the <code>id</code> command users can identify all the groups they belong to. The contents of /projects are cached on the local disk, read access against data in /projects will typically no place too much of burden on the file server. The contents of the <code>/projects</code> folder will not be completely listed when e.g. executing <code>ls /projects/</code> as contents are mounted on demand by automounter. You can request a <code>/project</code> directory by talking to us on Mattermost or have your PI request one.</p> <p>The <code>/groups</code> directory is identical to <code>/projects</code> in technology. However every group on the organization has their own subdirectory.</p>"},{"location":"transfer/","title":"Transferring data to the IKIM cluster","text":"<p><code>Nota bene</code>: The cluster can accomodate only de-identified data,  no directly patient related data can be uploaded. All PII personal identifying information has to be removed prior to upload.</p> <p>The cluster provices a number of storage facilities described here.</p>"},{"location":"transfer/#introduction-to-data-transfer","title":"Introduction to data transfer","text":"<p>Data transfer across the network offers a great way to avoid the use of sneakernet (i.e. carrying hard drivers around campus to move data from A to B). We strongly advise reading the info on the cluster storage prior to moving forward here.</p> <p>Larger scale data transfer requires some degree of familiarity with the technologies available and the sending and receiving systems.</p> <p>We provide three different means for data transfer. We note that for larger transfers, the speed of the device the data is stored on remotely makes a difference.</p>"},{"location":"transfer/#using-ssh-scp-to-move-data-into-the-cluster","title":"Using ssh / scp to move data into the cluster","text":"<p>In short on the remote system execute <code>tar -cpf - | ssh -J login.ikim.uk-essen.de shellhost.ikim.uk-essen.de \"tar -xpf -\"</code></p> <p>Read this for more details.</p>"},{"location":"transfer/#using-nc-to-move-data-into-the-cluster","title":"Using NC to move data into the cluster","text":"<p>In short:</p> <ul> <li>ensure NC is installed on the remote system</li> <li>you need to execute commands on both sending and receiving system On the receiving end, use this command:</li> </ul> <pre><code>cd /projects/&lt;MY RECEIVING DIRECTORY&gt;\nnc -vl 44444 | tar zxv\nnetcat-receiving-tar-gzipped-directory\n</code></pre> <p>On the sending end, use:</p> <pre><code>tar czp /path/to/directory/to/send | nc -N 10.11.12.10 44444\nnetcat-sending-tar-gzipped-directory\n</code></pre> <p>Read this this</p>"},{"location":"transfer/#how-to-pick-a-path-here","title":"How to pick a path here","text":"<p>Depending on your needs and the systems involved, your technology choices may vary. The table below might help pick the right path.</p> approach size limit number of files comment browser 500 GB &lt;100 easy to use ssh/scp 5TB unlimited use tar to group files nc unlimited unlimited complicated, use zip or tar to group files"},{"location":"transfer/#miscellaneous-comments","title":"Miscellaneous comments","text":"<p>The local storage on each node typically consists of a system partition and a data partition.</p>"},{"location":"transfer/#uploading-data-to-the-european-genome-phenome-archive-ega","title":"Uploading data to the European Genome-phenome Archive (EGA)","text":"<p>The European Genome-phenome Archive (EGA) is a service for permanent archiving and sharing of personally identifiable genetic, phenotypic, and clinical data generated for the purposes of biomedical research projects or in the context of research-focused healthcare systems.</p> <p>Thus, you can archive patient-related research data that you use in publications, and provide it to other scientists. Importantly, you can controll access and bind other users of the data to any conditions necessary to conform to the patient consent you originally obtained.</p> <p>There are different paths to upload data to different European Genome-phenome Archive (EGA) server locations, and each can be used from different interfaces to then provide the project and sample metadata. Please refer to the EGA submission documentation for up to date details on the different pathways. Here, we only document working ways of doing the data upload:</p>"},{"location":"transfer/#sftp-upload-to-the-ega-inbox","title":"SFTP upload to the EGA Inbox","text":"<p>The SFTP upload to the EGA Inbox should work as described in the EGA submission documentation. However, this restricts Metadata submission to the Submitter Portal, which is not documented beyond the obvious features. So if anything doesn't work there, you cannot finish your submission and might have to wait weeks for the HelpDesk to respond.</p>"},{"location":"transfer/#ftp-upload-to-ega","title":"FTP upload to EGA","text":"<p>For this pathway, make sure to first encrypt your data with EGACryptor, as described in the EGA docs. The only tool we got working for FTP upload is <code>LFTP</code>, however not as described in the EGA docs. Instead, the following set of commands should get a working FTP connection established:</p> <pre><code>lftp # this just starts the tool and sends you to an lftp prompt, all the following commands are within lftp\nset ftp:ssl-allow 0\nopen ftp.ega.ebi.ac.uk\nUSER &lt;ega_user_name&gt;\n</code></pre> <p>This should ask for your password and after successful login you should be able to use all the standard <code>lftp</code> commands, for example <code>ls</code> to query the remote directory or <code>mput</code> to upload multiple files. With this upload route, you should then be able to use the programmatic metadata submission via Webin.</p>"},{"location":"troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/#a-program-fails-with-the-message-too-many-open-files","title":"A program fails with the message <code>too many open files</code>","text":"<p>This limit can be changed for the current user session with <code>ulimit -Sn</code> followed by the desired number. For example:</p> <pre><code>ulimit -Sn 4096\n</code></pre> <p>It is advisable to execute the <code>ulimit</code> command only when a workflow requires keeping many files open at the same time. For example:</p> <pre><code>ulimit -Sn 4096 &amp;&amp; python mywork.py\n</code></pre>"},{"location":"troubleshooting/#fixing-permissions-for-shared-files","title":"Fixing permissions for shared files","text":"<p>If the permissions of files that are supposed to be shared are too restrictive, ask the owner to extend the permissions with:</p> <pre><code># Allow everyone to read everything in a directory.\nchmod -R a+r &lt;path to directory&gt;\n\n# Allow the owner's group to modify every file in a directory.\nchmod -R g+w &lt;path to directory&gt;\n\n# Extend the execution and directory browsing permissions that the owner has in a directory to everyone.\nfind &lt;path to directory&gt; -executable -exec chmod a+x {} \\;\n</code></pre>"},{"location":"troubleshooting/#are-gpu-drivers-installed-everywhere","title":"Are GPU drivers installed everywhere?","text":"<p>Yes, but you might need to install CUDA in your environment:</p> <p>Enrico writes that</p> <pre><code>the cuda version output from nvidia-smi doesn't mean necessarily that cuda is installed. It simply tells you which cuda release matches the installed drivers```\n\non slurm nodes, cuda is not preinstalled because I encourage people to install it in their own conda environments so that they can keep it stable\n\nOn non-slurm nodes, i.e. g1-7 and g1-9 currently, cuda is still preinstalled\n</code></pre>"},{"location":"troubleshooting/#gpu-memory-is-held-by-orphan-processes","title":"GPU memory is held by orphan processes","text":"<p>If processes are holding GPU memory while their parent terminates abnormally, they could be left with claimed memory and no work to do. In such a case, <code>nvidia-smi</code> will display occupied memory with no associated processes. The <code>fuser</code> command can help perform cleanup and reclaim GPU memory.</p> <p>For example, if <code>nvidia-smi</code> reports memory allocated in GPU 0, <code>fuser -v /dev/nvidia0</code> can display which processes are accessing it.</p> <pre><code>                USER    PID ACCESS COMMAND\n/dev/nvidia0:   bob  869108 F...m python\n                bob  1236874 F...m python\n                bob  1236922 F...m python\n</code></pre> <p>Users can only see processes owned by themselves. Administrators can run <code>fuser</code> as root to see all processes.</p> <p><code>fuser -k</code> sends a termination signal to the listed processes. It can be convenient as a last resort if certain processes are difficult to terminate cleanly.</p> <p>A typical troubleshooting sessions against leftover GPU memory might be as follows:</p> <ol> <li>Examine the output of <code>nvidia-smi</code> and look for GPUs with allocated memory but no processes attached.</li> <li>Execute <code>fuser -v /dev/nvidiaN</code> to list processes owned by you accessing the device (replace <code>N</code> with a GPU index).</li> <li>Save all important work, then try closing open programs cleanly.</li> <li>Execute <code>fuser -v /dev/nvidiaN</code> again.   If any processes show up and you don't have a way to terminate them cleanly, you can use <code>fuser -k /dev/nvidiaN</code> to kill them bluntly.</li> <li>Examine the output of <code>nvidia-smi</code> again.    If GPU memory is still occupied, ask an administrator to look into processes owned by other users.</li> </ol>"}]}